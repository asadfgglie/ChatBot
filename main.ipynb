{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizerFast, \n",
    "    AutoModelForCausalLM, \n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerFast, \n",
    "    DataCollatorForLanguageModeling,\n",
    "    BatchEncoding,\n",
    "    GPT2LMHeadModel,\n",
    "    GenerationConfig,\n",
    "    get_scheduler,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from typing import Tuple\n",
    "import random\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager-core).\n",
      "Your token has been saved to E:\\huggingface_cache\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese')\n",
    "\n",
    "train_dataset = load_dataset('asadfgglie/lccc_base_zh', use_auth_token=True, split='validation')\n",
    "test_dataset = load_dataset('asadfgglie/lccc_base_zh', use_auth_token=True, split='test')\n",
    "\n",
    "def tokenize_function(example) -> BatchEncoding:\n",
    "    return tokenizer(''.join('[SEP]'.join(example[\"dialog\"]).split()), truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_checkpoints(args: TrainingArguments, use_mtime=False) -> list[str]:\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format('checkpoint')))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format('checkpoint'), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "def rotate_checkpoints(args: TrainingArguments, use_mtime=False) -> None:\n",
    "    \"\"\"Check if we should delete older checkpoint(s)\"\"\"\n",
    "    if args.save_total_limit is None:\n",
    "        return\n",
    "    if args.save_total_limit <= 0:\n",
    "        return\n",
    "\n",
    "    checkpoints_sorted = sorted_checkpoints(args, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
    "    \n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    max_grad_norm=1.0,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=3,\n",
    "    output_dir='model_log',\n",
    "    logging_dir='model_log/log',\n",
    "    max_steps=10,\n",
    "    logging_first_step=True,\n",
    "    no_cuda=True,\n",
    "    evaluation_strategy='steps',\n",
    "    do_train=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    seed=random.randint(0, 999),\n",
    "    save_total_limit=2,\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "args.max_eval_steps = 3\n",
    "args.should_continue = False\n",
    "args.tokenizer_path_or_name = 'bert-base-chinese'\n",
    "args.model_path_or_name = 'ckiplab/gpt2-base-chinese'\n",
    "logger.debug(\"Training/evaluation parameters %s\", str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(args.tokenizer_path_or_name)\n",
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(args.model_path_or_name)\n",
    "\n",
    "config = GenerationConfig.from_model_config(model.generation_config)\n",
    "config.max_new_tokens=50\n",
    "config.do_sample=True\n",
    "config.top_k=50\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "model.generation_config = config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = tokenized_train_dataset[:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in ['dialog']}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator(samples[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train without Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s -%(levelname)s- %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y/%m/%d %H:%M:%S\",\n",
    "        level=logging.INFO, force=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(args.output_dir) and os.listdir(args.output_dir) \n",
    "    and args.do_train \n",
    "    and args.overwrite_output_dir\n",
    "    and not args.should_continue):\n",
    "    for checkpoint in sorted_checkpoints(args):\n",
    "        shutil.rmtree(checkpoint)\n",
    "    os.remove(os.path.join(args.output_dir, 'eval_results.txt'))\n",
    "    shutil.rmtree(args.logging_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns('dialog')\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_train_dataset, shuffle=True, batch_size=args.per_device_train_batch_size, collate_fn=data_collator)\n",
    "\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns('dialog')\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, shuffle=True, batch_size=args.per_device_eval_batch_size, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = min(args.num_train_epochs * len(train_dataloader), args.max_steps) if args.max_steps > 0 else args.num_train_epochs * len(train_dataloader)\n",
    "lr_scheduler: LambdaLR = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    if not args.no_cuda:\n",
    "        model.to(device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "except:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(max_eval_steps=0) -> dict[str, Tensor]:\n",
    "    eval_output_dir = args.output_dir\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "    eval_loss = 0.0\n",
    "    model.eval()\n",
    "    \n",
    "    batch_bar = tqdm(leave=False, iterable=range(min(max_eval_steps, len(test_dataloader)) if max_eval_steps > 0 else len(test_dataloader)), desc=\"Batch evaluating step\")\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        if batch['input_ids'].shape[1] > 1024: continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "            loss: Tensor = outputs.loss\n",
    "            eval_loss += loss.mean().item()\n",
    "        \n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        batch_bar.update()\n",
    "        \n",
    "        if max_eval_steps > 0 and nb_eval_steps > max_eval_steps:\n",
    "            break\n",
    "    \n",
    "    batch_bar.close()\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        for key in sorted(result.keys()):\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.path.exists(args.output_dir) and os.listdir(args.output_dir) \n",
    "    and args.do_train \n",
    "    and not args.overwrite_output_dir\n",
    "    and not args.should_continue):\n",
    "    raise ValueError(\n",
    "        \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.should_continue:\n",
    "    _sorted_checkpoints = sorted_checkpoints(args)\n",
    "    if len(sorted_checkpoints) == 0:\n",
    "        raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
    "    else:\n",
    "        args.model_path_or_name = _sorted_checkpoints[-1]\n",
    "        args.model_train_step_from = int(_sorted_checkpoints[-1].split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(args.tokenizer_path_or_name)\n",
    "model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(args.model_path_or_name)\n",
    "\n",
    "config = GenerationConfig.from_model_config(model.generation_config)\n",
    "config.max_new_tokens=50\n",
    "config.do_sample=True\n",
    "config.top_k=50\n",
    "config.pad_token_id=tokenizer.pad_token_id\n",
    "model.generation_config = config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if saved optimizer or scheduler states exist\n",
    "if (\n",
    "    args.model_path_or_name\n",
    "    and os.path.isfile(os.path.join(args.model_path_or_name, \"optimizer.pt\"))\n",
    "    and os.path.isfile(os.path.join(args.model_path_or_name, \"scheduler.pt\"))\n",
    "):\n",
    "    # Load in optimizer and scheduler states\n",
    "    optimizer.load_state_dict(torch.load(os.path.join(args.model_path_or_name, \"optimizer.pt\")))\n",
    "    lr_scheduler.load_state_dict(torch.load(os.path.join(args.model_path_or_name, \"scheduler.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 00:24:13 -INFO- __main__ - ***** Running training *****\n",
      "2023/03/19 00:24:13 -INFO- __main__ -   Num examples = 20000\n",
      "2023/03/19 00:24:13 -INFO- __main__ -   Num Epochs = 1\n",
      "2023/03/19 00:24:13 -INFO- __main__ -   Total optimization steps = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16846777a55f41409d5bf3d2a4a2f658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdfdc894903413ca0d1518f5e82c33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch training step:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6107e1cecf4fe8ad30367b08303abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total training step:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 00:24:52 -INFO- __main__ - loss: 5.4634772936503095\n",
      "2023/03/19 00:24:52 -INFO- __main__ - lr: [5.e-06]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7019a12f347c4fca974b04ffe69ffd74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch evaluating step:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 00:25:06 -INFO- __main__ - eval_perplexity: 163.0620880126953\n",
      "2023/03/19 00:25:33 -INFO- __main__ - loss: 5.02346404393514\n",
      "2023/03/19 00:25:33 -INFO- __main__ - lr: [0.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057d900b5c434d2e864f7b1987beee02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch evaluating step:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 00:25:47 -INFO- __main__ - eval_perplexity: 202.59303283691406\n",
      "2023/03/19 00:26:12 -INFO- __main__ - loss: 5.480917135874431\n",
      "2023/03/19 00:26:12 -INFO- __main__ - lr: [0.]\n",
      "2023/03/19 00:26:14 -INFO- __main__ - Deleting older checkpoint [model_log\\checkpoint-3] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e14d85a79b42479b5af06f5b0e15bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch evaluating step:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/19 00:26:22 -INFO- __main__ - eval_perplexity: 158.9920654296875\n",
      "2023/03/19 00:26:42 -INFO- __main__ - Saving model checkpoint to model_log\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "logger.info(\"  Total optimization steps = %d\", num_training_steps)\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=args.logging_dir)\n",
    "\n",
    "epoch_bar = trange(args.num_train_epochs, desc=\"Epoch\")\n",
    "batch_bar = tqdm(train_dataloader, desc=\"Batch training step\")\n",
    "progress_bar = trange(num_training_steps, desc=\"Total training step\")\n",
    "\n",
    "global_step = 0\n",
    "train_loss = 0\n",
    "logging_loss = 0\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    for batch in batch_bar:\n",
    "        progress_bar.update()\n",
    "        global_step += 1\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        if batch['input_ids'].shape[1] > 1024: continue\n",
    "\n",
    "        model.train()\n",
    "        outputs = model(**batch)\n",
    "        loss: Tensor = outputs.loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.to('cpu').item()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "            logger.info(f\"loss: {(train_loss - logging_loss) / args.logging_steps}\")\n",
    "            logger.info(f'lr: {np.array(lr_scheduler.get_last_lr())}')\n",
    "\n",
    "            tb_writer.add_scalar(\"lr\", np.array(lr_scheduler.get_last_lr()), global_step)\n",
    "            tb_writer.add_scalar(\"loss\", (train_loss - logging_loss) / args.logging_steps, global_step)\n",
    "            logging_loss = train_loss\n",
    "\n",
    "            \n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "            torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "            torch.save(lr_scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "            rotate_checkpoints(args)\n",
    "        \n",
    "        if (args.evaluation_strategy == 'steps' \n",
    "            and args.eval_steps is not None \n",
    "            and global_step % args.eval_steps == 0 \n",
    "            and global_step > args.eval_delay):\n",
    "            results = evaluate(args.max_eval_steps)\n",
    "            for key, value in results.items():\n",
    "                logger.info(f\"eval_{key}: {value.item()}\")\n",
    "                tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "        \n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            break\n",
    "\n",
    "    if args.evaluation_strategy == 'epoch' and epoch > args.eval_delay:\n",
    "        results = evaluate(args.max_eval_steps)\n",
    "        for key, value in results.items():\n",
    "            logger.info(f\"eval_{key}: {value.item()}\")\n",
    "            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "    \n",
    "    if args.max_steps > 0 and global_step > args.max_steps:\n",
    "        break\n",
    "    \n",
    "    batch_bar.reset()\n",
    "\n",
    "tb_writer.close()\n",
    "\n",
    "batch_bar.close()\n",
    "epoch_bar.close()\n",
    "progress_bar.close()\n",
    "\n",
    "\n",
    "# Create output directory if needed\n",
    "os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "model_to_save = (\n",
    "    model.module if hasattr(model, \"module\") else model\n",
    ")  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(args.output_dir)\n",
    "tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "# Good practice: save your training arguments together with the trained model\n",
    "torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='model',\n",
    "    no_cuda=False,\n",
    "    seed=random.randint(0, 999),\n",
    ")\n",
    "args.tokenizer_path_or_name = 'bert-base-chinese'\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model: model\n"
     ]
    }
   ],
   "source": [
    "model_name = args.output_dir\n",
    "try:\n",
    "    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(model_name, padding_side='left')\n",
    "except:\n",
    "    model_name = sorted_checkpoints(args)[-1]\n",
    "\n",
    "try:\n",
    "    model: PreTrainedModel = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer: BertTokenizerFast = BertTokenizerFast.from_pretrained(model_name, padding_side='left')\n",
    "except:\n",
    "    raise ValueError(f'There\\'s no model in {args.output_dir}')\n",
    "\n",
    "print(f'load model: {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() and not args.no_cuda else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 我好帥\n",
      "GPT: 可以啊\n",
      "User: 沒錯\n",
      "GPT: 沒錯啦\n",
      "User: 我最棒了\n",
      "GPT: 一次沒錯！\n",
      "User: 要不要去看電影?\n",
      "GPT: 有錯\n",
      "User: 哈哈\n",
      "GPT: 一起\n",
      "User: 甚麼完愣\n",
      "GPT: 別的時候我沒聊一下了\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "chat_history = []\n",
    "for step in range(6):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    chat_history.append(input(\"User:\"))\n",
    "    print(\"User:\", chat_history[-1])\n",
    "\n",
    "    user_inputs = tokenizer(tokenizer.sep_token + tokenizer.sep_token.join(chat_history), return_tensors='pt')\n",
    "    user_inputs: dict[str, Tensor] = {k: v.to(device) for (k, v) in user_inputs.items()}\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(generation_config=config, **user_inputs)\n",
    "    \n",
    "    chat_history.append(''.join(tokenizer.decode(chat_history_ids[:, user_inputs['input_ids'].shape[-1]:][0], skip_special_tokens=True).split()))\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"GPT: {}\".format(chat_history[-1]), flush=True)\n",
    "\n",
    "user_inputs = {k: v.to('cpu') for (k, v) in user_inputs.items()}\n",
    "del user_inputs\n",
    "model.to('cpu')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': '', 'overwrite_output_dir': False, 'do_train': False, 'do_eval': False, 'do_predict': False, 'evaluation_strategy': <IntervalStrategy.NO: 'no'>, 'prediction_loss_only': False, 'per_device_train_batch_size': 8, 'per_device_eval_batch_size': 8, 'per_gpu_train_batch_size': None, 'per_gpu_eval_batch_size': None, 'gradient_accumulation_steps': 1, 'eval_accumulation_steps': None, 'eval_delay': 0, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 3.0, 'max_steps': -1, 'lr_scheduler_type': <SchedulerType.LINEAR: 'linear'>, 'warmup_ratio': 0.0, 'warmup_steps': 0, 'log_level': 'passive', 'log_level_replica': 'passive', 'log_on_each_node': True, 'logging_dir': 'runs\\\\Mar19_18-59-20_DESKTOP-asadfgglie', 'logging_strategy': <IntervalStrategy.STEPS: 'steps'>, 'logging_first_step': False, 'logging_steps': 500, 'logging_nan_inf_filter': True, 'save_strategy': <IntervalStrategy.STEPS: 'steps'>, 'save_steps': 500, 'save_total_limit': None, 'save_on_each_node': False, 'no_cuda': False, 'use_mps_device': False, 'seed': 42, 'data_seed': None, 'jit_mode_eval': False, 'use_ipex': False, 'bf16': False, 'fp16': False, 'fp16_opt_level': 'O1', 'half_precision_backend': 'auto', 'bf16_full_eval': False, 'fp16_full_eval': False, 'tf32': None, 'local_rank': -1, 'xpu_backend': None, 'tpu_num_cores': None, 'tpu_metrics_debug': False, 'debug': [], 'dataloader_drop_last': False, 'eval_steps': None, 'dataloader_num_workers': 0, 'past_index': -1, 'run_name': '', 'disable_tqdm': False, 'remove_unused_columns': True, 'label_names': None, 'load_best_model_at_end': False, 'metric_for_best_model': None, 'greater_is_better': None, 'ignore_data_skip': False, 'sharded_ddp': [], 'fsdp': [], 'fsdp_min_num_params': 0, 'fsdp_transformer_layer_cls_to_wrap': None, 'deepspeed': None, 'label_smoothing_factor': 0.0, 'optim': <OptimizerNames.ADAMW_HF: 'adamw_hf'>, 'optim_args': None, 'adafactor': False, 'group_by_length': False, 'length_column_name': 'length', 'report_to': ['tensorboard'], 'ddp_find_unused_parameters': None, 'ddp_bucket_cap_mb': None, 'dataloader_pin_memory': True, 'skip_memory_metrics': True, 'use_legacy_prediction_loop': False, 'push_to_hub': False, 'resume_from_checkpoint': None, 'hub_model_id': None, 'hub_strategy': <HubStrategy.EVERY_SAVE: 'every_save'>, 'hub_token': None, 'hub_private_repo': False, 'gradient_checkpointing': False, 'include_inputs_for_metrics': False, 'fp16_backend': 'auto', 'push_to_hub_model_id': None, 'push_to_hub_organization': None, 'push_to_hub_token': None, 'mp_parameters': '', 'auto_find_batch_size': False, 'full_determinism': False, 'torchdynamo': None, 'ray_scope': 'last', 'ddp_timeout': 1800, 'torch_compile': False, 'torch_compile_backend': None, 'torch_compile_mode': None, '_n_gpu': 1, '__cached__setup_devices': device(type='cuda', index=0), 'b': 1}\n"
     ]
    }
   ],
   "source": [
    "class a:\n",
    "    def __init__(self) -> None:\n",
    "        self.a = 0\n",
    "    def __str__(self) -> str:\n",
    "        return str(vars(self))\n",
    "t=TrainingArguments(output_dir='')\n",
    "t.b=1\n",
    "print(vars(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'tmp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tmp' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BertTokenizerFast\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m'\u001b[39;49m, padding_side\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1788\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[0;32m   1783\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1784\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1785\u001b[0m     )\n\u001b[0;32m   1787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1788\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1789\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1790\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1791\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1792\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1793\u001b[0m     )\n\u001b[0;32m   1795\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   1796\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'tmp'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'tmp' is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "BertTokenizerFast.from_pretrained('tmp', padding_side='left')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6effec126ee2855b34f08a687b2d6b94cd3e41f804cf6a09ac676ca926f2ce1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
