{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More information about `datasets.Dataset`, go to https://huggingface.co/docs/datasets/access\n",
    "\n",
    "#### More information about `opencc`, go to https://clay-atlas.com/blog/2019/09/24/python-chinese-tutorial-opencc/\n",
    "opencc不一定能完整轉換臺灣/中國慣用詞，為此請使用 `resourse/word.csv` 額外詞典輔助轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, AutoModelForCausalLM\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese', padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration asadfgglie--lccc_base_zh-fb19318066f5fd62\n",
      "Found cached dataset parquet (E:/huggingface_cache/datasets/asadfgglie___parquet/asadfgglie--lccc_base_zh-fb19318066f5fd62/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "tmp = load_dataset('asadfgglie/lccc_base_zh', use_auth_token=True, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    for k,i in enumerate(example[\"dialog\"]):\n",
    "        example[\"dialog\"][k] = tokenizer(''.join('[SEP]'.join(i).split()), truncation=True)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration asadfgglie--lccc_base_zh-fb19318066f5fd62\n",
      "Found cached dataset parquet (E:/huggingface_cache/datasets/asadfgglie___parquet/asadfgglie--lccc_base_zh-fb19318066f5fd62/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('asadfgglie/lccc_base_zh', use_auth_token=True, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c38183bc784abea27011c8bee11a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6821 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenized_train_dataset \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mmap(tokenize_function, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m      2\u001b[0m tokenized_train_dataset\u001b[39m.\u001b[39mset_format(\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:2815\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2812\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2814\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 2815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   2816\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   2817\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   2818\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   2819\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   2820\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   2821\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2822\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   2823\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   2824\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   2825\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   2826\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   2827\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   2828\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   2829\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   2830\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   2831\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   2832\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   2833\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   2834\u001b[0m     )\n\u001b[0;32m   2835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2837\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:546\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    547\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    548\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    549\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:513\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    506\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    507\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    508\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    509\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    510\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    511\u001b[0m }\n\u001b[0;32m    512\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    514\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    515\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    478\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    482\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:3236\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   3232\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   3233\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   3234\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3236\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3237\u001b[0m         batch,\n\u001b[0;32m   3238\u001b[0m         indices,\n\u001b[0;32m   3239\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   3240\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   3241\u001b[0m     )\n\u001b[0;32m   3242\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3243\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3244\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3245\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\datasets\\arrow_dataset.py:3112\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   3111\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> 3112\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3114\u001b[0m     processed_inputs \u001b[39m=\u001b[39m {\n\u001b[0;32m   3115\u001b[0m         k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m processed_inputs\u001b[39m.\u001b[39mkeys_to_format\n\u001b[0;32m   3116\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(example)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_function\u001b[39m(example):\n\u001b[0;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m k,i \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(example[\u001b[39m\"\u001b[39m\u001b[39mdialog\u001b[39m\u001b[39m\"\u001b[39m]):\n\u001b[1;32m----> 3\u001b[0m         example[\u001b[39m\"\u001b[39m\u001b[39mdialog\u001b[39m\u001b[39m\"\u001b[39m][k] \u001b[39m=\u001b[39m tokenizer(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39m[SEP]\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(i)\u001b[39m.\u001b[39;49msplit()), truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m example\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2522\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2523\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_one(text\u001b[39m=\u001b[39mtext, text_pair\u001b[39m=\u001b[39mtext_pair, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs)\n\u001b[0;32m   2524\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2609\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2610\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2611\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2626\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2627\u001b[0m     )\n\u001b[0;32m   2628\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2630\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2631\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2632\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2633\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2634\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2635\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2636\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2637\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2638\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2639\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2640\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2641\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2642\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2643\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2644\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2645\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2646\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2647\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2648\u001b[0m     )\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2702\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2693\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2694\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2695\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2699\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2700\u001b[0m )\n\u001b[1;32m-> 2702\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_plus(\n\u001b[0;32m   2703\u001b[0m     text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2704\u001b[0m     text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[0;32m   2705\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2706\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2707\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2708\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2709\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2710\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2711\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2712\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2713\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2714\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2715\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2716\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2717\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2718\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2719\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2720\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2721\u001b[0m )\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[1;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[0;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[0;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[1;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m    503\u001b[0m         batched_input,\n\u001b[0;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    509\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    519\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[0;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[0;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[0;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[1;32me:\\Enviroment\\AI\\TF2_Pytorch\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[0;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m    427\u001b[0m )\n\u001b[1;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[0;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[0;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[0;32m    433\u001b[0m )\n\u001b[0;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[0;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[0;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[0;32m    453\u001b[0m ]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, batch_size=1000)\n",
    "tokenized_train_dataset.set_format(\"torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6effec126ee2855b34f08a687b2d6b94cd3e41f804cf6a09ac676ca926f2ce1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
